{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"CrawlingwithScrapy.ipynb","provenance":[],"collapsed_sections":[],"authorship_tag":"ABX9TyPnoSjnhx1FTZWQ4RpThFQ9"},"kernelspec":{"name":"python3","display_name":"Python 3"}},"cells":[{"cell_type":"markdown","metadata":{"id":"6k0Y935r-ad3"},"source":["# Scrapy\n","- Scrapy is a Python library that handles much of the complexity of finding and evaluating links on a website, crawling domains or lists of domains with ease."]},{"cell_type":"code","metadata":{"id":"xKsE_vQd_Rp-"},"source":[" pip install scrapy"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"3fAUdObDAMPH"},"source":["### What’s the difference between an API and a regular website? \n","\n","- Despite the hype around APIs, the answer is often: not much. APIs function via HTTP, the same protocol used to fetch data for websites, download a file, and do almost anything else on the Internet.\n","\n","-  The only things that makes an API an API is the extremely regulated syntax it uses, and the fact that APIs present their data as JSON or XML, rather than HTML."]},{"cell_type":"markdown","metadata":{"id":"Ii_UQssLCASH"},"source":["### Methods\n","There are four ways to request information from a web server via HTTP:\n","\n","  - <b>GET</b> : GET is what you use when you visit a website through the address bar in your browser. You can think of GET as saying, “Hey, web server, please get me this information.”\n","\n","  - <b>POST</b> : POST is what you use when you fill out a form, or submit information, presumably to a backend script on the server. Every time you log into a website, you are making a POST request with your username and (hopefully) encrypted password. <br>\n","      \n","      If you are making a POST request with an API, you are saying, “Please store this information in your database.”\n","\n","  - <b>PUT </b> :  PUT is less commonly used when interacting with websites, but is used from time to time in APIs. A PUT request is used to update an object or information. <br>\n","  \n","      An API might require a POST request to create a new user, for example, but it might need a PUT request if you want to update that user’s email address.\n","\n","\n","  - <b>DELETE </b> :DELETE is straightforward; it is used to delete an object. For instance, if I send a DELETE request to http://myapi.com/user/23, it will delete the user with the ID 23. \n","  \n","  DELETE methods are not often encountered in public APIs, which are primarily created to disseminate information rather than allow random users to remove that information from their databases. However, like the PUT method, it’s a good one to know about.\n"]},{"cell_type":"markdown","metadata":{"id":"1hug_aBNDGB5"},"source":["### Responses\n","\n","- An important feature of APIs is that they have well-formatted responses. The most common types of response formatting are eXtensible Markup Language (XML) and JavaScript Object Notation (JSON).\n","\n","- In recent years, JSON has become vastly more popular than XML for a couple of major reasons. First, JSON files are generally smaller than well-designed XML files.\n","\n","- Another reason JSON is quickly becoming more popular than XML is simply due to a shift in web technologies. In the past, it was more common for a server-side script such as PHP or .NET to be on the receiving end of an API.\n","\n","- Nowadays, it is likely that a framework, such as Angular or Backbone, will be sending and receiving API calls. \n","\n","- Server-side technologies are somewhat agnostic as to the form in which their data comes. But JavaScript libraries like Backbone find JSON easier to handle.\n","\n","- Although most APIs still support XML output, we will be using JSON examples in this book. Regardless, it is a good idea to familiarize yourself with both if you haven’t already — they are unlikely to go away any time soon."]},{"cell_type":"markdown","metadata":{"id":"CBuf-uc4Shnh"},"source":["### Echo Nest\n","\n","- The Echo Nest is a fantastic example of a company that is built on web scrapers. Although some music-based companies, such as Pandora, depend on human intervention to categorize and annotate music, \n","\n","- The Echo Nest relies on automated intelligence and information scraped from blogs and news articles in order to categorize musical artists, songs, and albums.\n","\n","- Even better, this API is freely available for noncommercial use.3 You cannot use the API without a key, but you can obtain a key by going to The Echo Nest “Create an Account” page and registering with a name, email address, and username."]},{"cell_type":"markdown","metadata":{"id":"LmNpMep5S7ZP"},"source":["#### A Few Examples\n","\n","- The Echo Nest API is built around several basic content types: artists, songs, tracks, and genres. Except for genres, these content types all have unique IDs, which are used to retrieve information about them in various forms, through API calls. \n","\n","- For example, if I wanted to retrieve a list of songs performed by Monty Python, I would make the following call to retrieve their ID (remember to replace <your api key> with your own API key):\n","\n","\n","- http://developer.echonest.com/api/v4/artist/search?api_key=<your api key >&name=monty%20python\"\n"]},{"cell_type":"markdown","metadata":{"id":"B_VIkQr_UpFM"},"source":["### Twitter\n","\n","- Twitter is notoriously protective of its API and rightfully so. With over 230 million active users and a revenue of over $100 million a month, the company is hesitant to let just anyone come along and have any data they want.\n","\n","- Twitter’s rate limits (the number of calls it allows each user to make) fall into two categories: 15 calls per 15-minute period, and 180 calls per 15-minute period, depending on the type of call. For instance, you can make up to 12 calls a minute to retrieve basic information about Twitter users, but only one call a minute to retrieve lists of those users’ Twitter followers.4"]},{"cell_type":"markdown","metadata":{"id":"YsrdV7SkVtgA"},"source":["### Google API\n","\n","- Google has one of the most comprehensive, easy-to-use collections of APIs on the Web today. Whenever you’re dealing with some sort of basic subject, such as language translation, geolocation, calendars, or even genomics, Google has an API for it. \n","\n","- Google also has APIs for many of its popular apps, such as Gmail, YouTube, and Blogger"]},{"cell_type":"markdown","metadata":{"id":"uWhv8Yg3VDpn"},"source":["### A Few Examples\n","\n","- Google’s most popular (and in my opinion most interesting) APIs can be found in its collection of Maps APIs. You might be familiar with this feature through the embeddable Google Maps found on many websites. \n","\n","- However, the Maps APIs go far beyond embedding maps — you can resolve street addresses to latitude/longitude coordinates, get the elevation of any point on Earth, create a variety of location-based visualizations, and get time zone information for an arbitrary location, among other bits of information.\n","\n","\n","- https://maps.googleapis.com/maps/api/geocode/json?address=1+Science+Park+Boston +MA+02114&key=your API key"]},{"cell_type":"markdown","metadata":{"id":"Pk7m5V1FVKcL"},"source":["- To get the time zone information for our newly found latitude and longitude, you can use the Time zone API:\n","\n","- https://maps.googleapis.com/maps/api/timezone/json?location=42.3677994,-71.0708 078&timestamp=1412649030&key=your API key\n"]},{"cell_type":"markdown","metadata":{"id":"8tfa9tu4ZFsu"},"source":["## Parsing JSON"]},{"cell_type":"code","metadata":{"id":"0FrkTgUSZd06"},"source":["import json\n","from urllib.request import urlopen\n","\n","def getCountry(ipAddress):\n","\n","  response = urlopen(\"https://freegeoip.app/json/\"+ipAddress).read()\\\n","                                                             .decode('utf-8') \n","  responseJson = json.loads(response)\n","\n","  return responseJson.get(\"country_code\")"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"fAHJg4OKaPZP","executionInfo":{"status":"ok","timestamp":1615736845474,"user_tz":-330,"elapsed":865,"user":{"displayName":"sami","photoUrl":"","userId":"18111817756020762677"}},"outputId":"ca23017f-86f9-4e47-fe4d-5038917e5e26"},"source":["print(getCountry(\"50.78.253.58\"))"],"execution_count":null,"outputs":[{"output_type":"stream","text":["US\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"xqjJqLTca7D1"},"source":["- The JSON parsing library used is part of Python’s core library. Just type in import json at the top, and you’re all set! \n","\n","- Unlike many languages that might parse JSON into a special JSON object or JSON node, Python uses a more flexible approach and turns JSON objects into dictionaries, JSON arrays into lists, JSON strings into strings, and so forth. \n","\n","- In this way, it makes it extremely easy to access and manipulate values stored in JSON."]},{"cell_type":"code","metadata":{"id":"EhjSIrlvbMvU"},"source":["import json\n","\n","jsonString = '''{\"arrayOfNums\":[{\"number\":0},{\"number\":1},{\"number\":2}],\n","                \"arrayOfFruits\":[{\"fruit\":\"apple\"},{\"fruit\":\"banana\"},\n","                                 {\"fruit\":\"pear\"}]}'''\n","\n","\n","jsonObj = json.loads(jsonString)\n"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"AU_-DZ9rbrui","executionInfo":{"status":"ok","timestamp":1615737271249,"user_tz":-330,"elapsed":800,"user":{"displayName":"sami","photoUrl":"","userId":"18111817756020762677"}},"outputId":"c23cfe63-f462-4be1-81e3-462fcbae0231"},"source":["print(jsonObj.get(\"arrayOfNums\")) \n","print(jsonObj.get(\"arrayOfNums\")[1]) \n","print(jsonObj.get(\"arrayOfNums\")[1].get(\"number\")+\n","      jsonObj.get(\"arrayOfNums\")[2].get(\"number\")) \n","\n","print(jsonObj.get(\"arrayOfFruits\")[2].get(\"fruit\"))"],"execution_count":null,"outputs":[{"output_type":"stream","text":["[{'number': 0}, {'number': 1}, {'number': 2}]\n","{'number': 1}\n","3\n","pear\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"NUMnMV5ocjBp"},"source":["### Bringing It All Back Home\n","\n","- Although the raison d'être of many modern web applications is to take existing data and format it in a more appealing way, I would argue that this isn’t very interesting thing to do in most instances. \n","\n","- If you’re using an API as your only data source, the best you can do is merely copy someone else’s database that already exists, and which is, essentially, already published. \n","\n","- What can be far more interesting is to take two or more data sources and combine them in a novel way, or use an API as a tool to look at scraped data from a new perspective."]},{"cell_type":"markdown","metadata":{"id":"LXE-cg8uchln"},"source":["#### Creating a basic script that crawls Wikipedia, looks for revision history pages, and then looks for IP addresses on those revision history pages"]},{"cell_type":"code","metadata":{"id":"XDbpYnTceGYF","executionInfo":{"status":"ok","timestamp":1615823112987,"user_tz":-330,"elapsed":1223,"user":{"displayName":"sami","photoUrl":"","userId":"18111817756020762677"}}},"source":["from urllib.request import urlopen \n","from bs4 import BeautifulSoup \n","import datetime\n","import random\n","import json\n","import re\n","\n","random.seed(datetime.datetime.now()) \n","\n","def getLinks(articleUrl):\n","  html = urlopen(\"http://en.wikipedia.org\"+articleUrl)\n","  bsObj = BeautifulSoup(html)\n","\n","  return bsObj.find(\"div\", {\"id\":\"bodyContent\"}).findAll(\"a\",\n","                                      href=re.compile(\"^(/wiki/)((?!:).)*$\"))"],"execution_count":27,"outputs":[]},{"cell_type":"code","metadata":{"id":"jdWyAew7ecup","executionInfo":{"status":"ok","timestamp":1615823114729,"user_tz":-330,"elapsed":1092,"user":{"displayName":"sami","photoUrl":"","userId":"18111817756020762677"}}},"source":["def getHistoryIPs(pageUrl):\n","\n","  #Format of revision history pages is: \n","  #http://en.wikipedia.org/w/index.php?title=Title_in_URL&action=history \n","\n","  pageUrl = pageUrl.replace(\"/wiki/\", \"\")\n","  historyUrl = \"http://en.wikipedia.org/w/index.php?title=\"+ \\\n","                                                      pageUrl+\"&action=history\" \n","  print(\"history url is: \"+historyUrl)\n","\n","  html = urlopen(historyUrl)\n","  bsObj = BeautifulSoup(html)\n","\n","  #finds only the links with class \"mw-anonuserlink\" which has IP addresses \n","  #instead of usernames\n","  ipAddresses = bsObj.findAll(\"a\", {\"class\":\"mw-anonuserlink\"}) \n","  addressList = set()\n","\n","  for ipAddress in ipAddresses:\n","    addressList.add(ipAddress.get_text())\n","\n","  return addressList"],"execution_count":28,"outputs":[]},{"cell_type":"code","metadata":{"id":"JgwOPw9OfXg7"},"source":["links = getLinks(\"/wiki/Python_(programming_language)\")\n","\n","while(len(links) > 0): \n","  for link in links:\n","    print(\"-------------------\")\n","    historyIPs = getHistoryIPs(link.attrs[\"href\"]) \n","    for historyIP in historyIPs:\n","      print(historyIP)\n","\n","  newLink = links[random.randint(0, len(links)-1)].attrs[\"href\"]\n","  links = getLinks(newLink)"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"WFzRIFkafNkr"},"source":["- This code also uses a somewhat arbitrary (yet effective for the purposes of this example) search pattern to look for articles from which to retrieve revision histories. \n","\n","- It starts by retrieving the histories of all Wikipedia articles linked to by the starting page (in this case, the article on the Python programming language). \n","\n","- Afterward, it selects a new starting page randomly, and retrieves all revision history pages of articles linked to by that page. It will continue until it hits a page with no links."]},{"cell_type":"code","metadata":{"id":"8iYDfNjThBaU","executionInfo":{"status":"ok","timestamp":1615823198183,"user_tz":-330,"elapsed":1033,"user":{"displayName":"sami","photoUrl":"","userId":"18111817756020762677"}}},"source":["def getCountry(ipAddress): \n","  try:\n","    response = urlopen(\"https://freegeoip.app/json/\"+ipAddress).read()\\\n","                                                                .decode('utf-8')\n","  except HTTPError: \n","    return None\n","  \n","  responseJson = json.loads(response) \n","  return responseJson.get(\"country_name\")"],"execution_count":32,"outputs":[]},{"cell_type":"code","metadata":{"id":"pGgmcsX8hZtT"},"source":["links = getLinks(\"/wiki/Python_(programming_language)\")\n","\n","while(len(links) > 0): \n","  for link in links:\n","    print(\"-------------------\")\n","    historyIPs = getHistoryIPs(link.attrs[\"href\"]) \n","\n","    for historyIP in historyIPs:\n","      country = getCountry(historyIP) \n","      if country is not None:\n","        print(historyIP+\" is from \"+country)\n","\n","  newLink = links[random.randint(0, len(links)-1)].attrs[\"href\"]\n","  links = getLinks(newLink)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"2Kd0geOYj72z"},"source":[""],"execution_count":null,"outputs":[]}]}